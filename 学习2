from urllib import request
resp = request.urlopen('http://www.baidu.com')
print(resp.read())
以上三行代码可以成功获取一页网页的全部信息。

urllib.request.Request (为获取的网址添加头部）
urllib.request.urlopen(请求获得一个网址）
urllib.request.urlretrieve（将网页上的一个文件保存到本地） 
urllib.parse.urlencode(把字典数据转换为url的编码数据）
urllib.parse.parse_qs(将编码后的url数据进行解码）
urllib.parse.urlparse/urllib.parse.urlsplit(把url的各个部分进行分割）

from urllib.request
url = 'http://www.baidu.com'
headers = {
'User-Agent':'名称'
}
rq = request.Request(url, headers = headers)
resp = request.urlopen(rq)
print(resp.read())

ProxyHandler处理器（代理）
1.代理的原理：在请求目的网站之前，现请求代理服务器，然后让代理服务器去请求目的网站，代理服务器拿到目的网站的数据后，在转发给我们的代码
2.http://httpbin.org:这个网站可以方便的查看http请求的一些参数
3.在代码中使用代理：
    使用urllib.request.ProxyHandler,传入一个代理，这个代理是一个字典，字典的kay依赖于代理服务器能够接收的类型，一般是'http' 'https',值是:'ip:port'
    使用上一步创建的'handler',以及'requset.build_opener'创建一个'opener'
    使用上一步创建的'opener',调用'open'函数，发起请求
示例代码：

from urllib import request
url = 'http://httpbin.org/ip'
handler = request.Proxyhandler({"http":"代理ip地址"})
opener = request.bulid_opener(handler)
resp = opener.open(url)
prit(resp.read())

以上内容为昨天的学习内容，就这几个库几个代码，记住就好了^_^
昨天给风电项目加无功补偿GG，很烦
昨天跑了4圈，走了4圈，大胆意淫了今后的生活
长点心，对事不对人，不要随意去评价他人，这一点很关键，在学会做人的基础上学会做事，加油↖(^ω^)↗
